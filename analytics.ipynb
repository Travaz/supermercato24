{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analytics - Linear Regression Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common end-to-end ML project schedule to take inspirations from\n",
    "\n",
    "    (1) Look at the Big Picture\n",
    "            Frame the Problem\n",
    "            Select a Performance Measure\n",
    "            Check the Assumptions\n",
    "    (2) Get the Data\n",
    "            Create the Workspace\n",
    "            Download the Data\n",
    "            Take a Quick Look at the Data Structure\n",
    "            Create a Test Set\n",
    "    (3) Discover and Visualize the Data to Gain Insights\n",
    "            Visualizing Geographical Data\n",
    "            Looking for Correlations\n",
    "            Experimenting with Attribute Combinations\n",
    "    (4) Prepare the Data for Machine Learning Algorithms\n",
    "            Data Cleaning\n",
    "            Handling Text and Categorical Attributes\n",
    "            Custom Transformers\n",
    "            Feature Scaling\n",
    "            Transformation Pipelines\n",
    "    (5) Select and Train a Model\n",
    "            Training and Evaluating on the Training Set\n",
    "            Better Evaluation Using Cross-Validation\n",
    "    (6) Fine-Tune Your Model\n",
    "            Grid Search\n",
    "            Randomized Search\n",
    "            Ensemble Methods (Random forest and Gradient Boosting)\n",
    "            Analyze the Best Models and Their Errors\n",
    "            Evaluate Your System on the Test Set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(TODO) My Index: \n",
    "\n",
    "    (1) Description of the challenge\n",
    "    (2) Data description \n",
    "    (3) Data visualization \n",
    "    (4) Data preprocessing -> Pipeline\n",
    "    (5) Scope definition\n",
    "    (6) Models definition\n",
    "    (7) Model-tuning and cross-validation for optimization!\n",
    "    (8) Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Challenge description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (TODO) Here we provide a little overview of what we are required to do with my own words (rephrase the problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here attached a csv file containing a sample dataset consisting in (x, y) pairs on which you’ll have to perform a regression task. Here’s what you’ll need to do:\n",
    "\n",
    "    1.\tOpen the file and read the data.\n",
    "    2.\tClean the data if needed.\n",
    "    3.\tPlot the data.\n",
    "    4.\tFit two different regression models to the data (e.g. curves belonging to two different families).\n",
    "    5.\tChoose a metric for the evaluation of the models and perform the evaluation using cross-validation.\n",
    "    6.\tPlot the predictions from the best model against the data.\n",
    "\n",
    "We ask you to work in a Jupyter notebook specifying the versions of Python (version 3.x please) and all libraries you use: when you’re done send us the notebook and we’ll execute it and check the results. As an alternative, you can also work in a script: just make sure the results from cross-validation are printed to screen when we execute it via the terminal, and that the plots get saved as files in png format in the same directory containing the script (send us a compressed archive with the script and the plots you produced).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Dataset overview [TODO]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (TODO) In this section we are going to load the data, conduct a preliminary analysis on the main characteristics of the data provided to get a sense on how we should proceed in our analysis (data to be cleaned? standardization vs. normalization? univariate vs. bivariate analysis?)\n",
    "\n",
    "    (1) Complete description\n",
    "    (2) See what we can add in order to perform a better analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"analytics_task_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "           x           y\n0   8.519119   48.073811\n1  12.059918  302.657224\n2  11.547357  247.531965\n3  10.383055  127.686318\n4   8.492261   55.086710",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8.519119</td>\n      <td>48.073811</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>12.059918</td>\n      <td>302.657224</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>11.547357</td>\n      <td>247.531965</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10.383055</td>\n      <td>127.686318</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8.492261</td>\n      <td>55.086710</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                x           y\ncount  488.000000  462.000000\nmean     9.767819  133.752630\nstd      1.614540   91.797160\nmin      7.007803   10.443932\n25%      8.380894   59.919619\n50%      9.781597  104.201239\n75%     11.122550  194.316855\nmax     12.498972  375.943890",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>488.000000</td>\n      <td>462.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>9.767819</td>\n      <td>133.752630</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.614540</td>\n      <td>91.797160</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>7.007803</td>\n      <td>10.443932</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>8.380894</td>\n      <td>59.919619</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>9.781597</td>\n      <td>104.201239</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>11.122550</td>\n      <td>194.316855</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>12.498972</td>\n      <td>375.943890</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "x    12\ny    38\ndtype: int64\n"
    }
   ],
   "source": [
    "# If there are missing values, we wish to see an overview for each column\n",
    "if np.array(df.isnull().any()).any() == True: # TODO: This code might need some improvements!\n",
    "    missing_data_sum = df.isnull().sum()\n",
    "    print(missing_data_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Percentage of missings over total for each column:\n\n        x: 2.4% \t \n        y: 7.6% \n\n"
    }
   ],
   "source": [
    "# Wish to go a little more in-depth with these missing values \n",
    "\n",
    "# Percentage of missings over total\n",
    "print(f\"\"\"Percentage of missings over total for each column:\n",
    "\n",
    "        x: {missing_data_sum.x/len(df.x) * 100}% \\t \n",
    "        y: {missing_data_sum.y/len(df.y) * 100}% \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given low percentages of data missing compared to overall size, we might want to proceed to drop them. However, we make a consideration here:\n",
    "# x -> drop (we might be to impute with sophisticated imputer such as KNN since a simpler one would probably get to a faulty road)\n",
    "# y -> drop them, but store on an external file that would work as a test data. Why? Well, because we might want to see how the two models perform on data that they have never seen before and plot how the line fit with these values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Data visualization [TODO]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (TODO) Data visualization is a fundamental step in order to further explore the relationship between predictor(s) and target variable\n",
    "\n",
    "    (1) Might need the course on Kaggle to do a good job here!\n",
    "    (2) Univariate vs. Bivariate analysis:\n",
    "            Boxplots\n",
    "            Scatter plots\n",
    "    (3) Remember to add a final evaluation of the curve on the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main visualization libraries\n",
    "import matplotlib.pyplot as plot\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=df.x,y=df.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First plot\n",
    "plt.figure(figsize=(16,6))\n",
    "sns.scatterplot(x=df.x,y=df.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Data preprocessing + (5) Data Modeling + (6) Parameter Optimizations and Cross-validation + (7) Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (TODO) To restructure!\n",
    "\n",
    "#### (TODO) In this section we are going to defined two different regression models based on the analysis we conducted so far. Based on the visualizations, we already know that the relationship between the two variables is not linear, and for this reason we can exclude a linear regression model which would then underfit our data. We need to add more complexity to the models, and we do by introducing non-linearity in the model: Polynomial!\n",
    "\n",
    "#### (TODO) Is the model we picked the best we could possibly pick? We might need to twist a little the parameters in order to get the best model and at the same time we wish to take care of underfitting vs. overfitting and bias-variance trade-off issues \n",
    "\n",
    "#### (TODO) MSE? RMSE? How should we evaluate the model performance? This might be included in cross-valudation and optimization of hyperparameters (such as the the polynomial line or regression!)\n",
    "\n",
    "#### (TODO) Based on the analysis we have conducted so far, we want to pick the best model that represents the relationship between the explanatory (x) and the dependent variable (y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (TODO) Data preprocessing step will enable us to get our hands on the data and to correct them in order to avoid as much as possible pitfalls in modeling   \n",
    "\n",
    "    (1) Drop vs. imputing\n",
    "    (2) Standardization? Normalization? -> based on the data visualization analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treating missings on the dependent variable (\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve indeces for null values\n",
    "na_y_indeces = df.y[df.y.isnull()].index\n",
    "\n",
    "# Store values of \"x\" (for which \"y\" is null) in a new dataframe with new keys\n",
    "x_test = pd.DataFrame({\"x\": df.x.loc[na_y_indeces].values})\n",
    "\n",
    "# Save the data in a new file -> analytics_task_data_test.csv\n",
    "x_test.to_csv(\"analytics_task_data_test.csv\",index=False)\n",
    "\n",
    "# Drop these values from the original dataframe\n",
    "df.dropna(axis=0,subset=[\"y\"], inplace=True)\n",
    "\n",
    "# Assert that all proceeded as planned\n",
    "assert np.in1d(df.x,x_test.x).any() == False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treat missings from explanatory variable (\"x\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st approach -> Drop them\n",
    "df.dropna(axis=0,subset=[\"x\"], inplace=True)\n",
    "\n",
    "#(TODO) -> Want to see whether such an approach improve performances, but I would already consider to drop the missing values for \"x\" anyway since they represent only the 2.7% of the overall sample. Moreover, the data provided already defines a clear trend (N.B.which might actually be present only in the sample data). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split between explanatory and predictor variable and reshaped to include another axis before splitting\n",
    "x = df.x.values.reshape(-1,1)\n",
    "y = df.y.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to pass the whole execution to a pipeline so that it is easier to replicate, mantain and to read!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# from sklearn.preprocessing import Normalizer # NOT YET!\n",
    "# from sklearn.preprocessing import StandardScaler # NOT YET!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for the high-degree polynomial model\n",
    "def get_poly_pipeline(degree):\n",
    "    # Polynomial-degree transformer with a degree of 2 (parameter to be optimized sooner or later!)\n",
    "    my_poly_transformer = PolynomialFeatures(degree=degree)\n",
    "    # Define the model\n",
    "    my_poly_model = LinearRegression()\n",
    "    # Define and return the pipeline: bundle together data preprocessing and model definition\n",
    "    return make_pipeline(my_poly_transformer,my_poly_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to optimize hyper-parameters for the polynomial linear regression model\n",
    "def get_poly_lr_score(poly_degree=2,kfold=5):\n",
    "    \"\"\"\n",
    "        Execute the following pipeline: Preprocessing, Model definition, Model fitting with cross validation \n",
    "\n",
    "            Input: \n",
    "                (1) poly_degree: set the highest degree polynomial in the regression equation\n",
    "                (2) kfold: set the number of splits performed by cross validation \n",
    " \n",
    "            Output:\n",
    "                (1) (TODO) the average score produced from cross validation with \n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve the pipeline for the polynomial regression model \n",
    "    my_pipeline = get_poly_pipeline(degree=poly_degree)\n",
    "    # Perform K-fold cross validation on the pipeline with a negative mean absolute error as evaluation metric for the model at each split\n",
    "    # (TODO) (N.B.) each scores is multiply by 1 in order to resotre the minimization problem: we want to get the lowest possible scores for that metric\n",
    "    scores = (-1)*cross_val_score(my_pipeline,x,y,cv=5,scoring = \"neg_mean_absolute_error\")\n",
    "    # (5) Return the score as an average of the MSE on different train-validation splits\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to balance the bias-variance tradeoff in order to avoid as much as possible the risk of overfitting or underfitting.\n",
    "# How? We use the function we have just defined for optimization purposes: we want to retrieve the best parameter for the model\n",
    "\n",
    "# Define a range of possible polynomial degrees that goes from 2 to 20 (i.e. in the polynomial regression equation, this would mean from x**2 to x**30 [highly non-linear relationship])\n",
    "degrees = range(2,30)\n",
    "# Define the average scores for each of the possible values\n",
    "scores = [get_poly_lr_score(degree) for degree in degrees]\n",
    "# Collect these in a dictionary with key-value pairs\n",
    "scores_dict = dict(zip(degrees,scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "plt.plot(list(scores_dict.keys()),list(scores_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to retrieve the best performing parameter for the model\n",
    "degree_best = min(scores_dict,key=scores_dict.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's define the final model and predict on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline definition and fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for the high-degree polynomial model\n",
    "my_poly_model_best = get_poly_pipeline(degree_best)\n",
    "# Fit the model on the whole dataset\n",
    "my_poly_model_best.fit(x,y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieving test data and producing predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "df_test = pd.read_csv(\"analytics_task_data_test.csv\")\n",
    "x_test = df_test.x.values.reshape(-1,1)\n",
    "\n",
    "# Pipeline the data in order to make preprocessing also on test data and predictions\n",
    "y_poly_preds = my_poly_model_best.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to plot with older curve!\n",
    "import operator\n",
    "\n",
    "# Normal plot!\n",
    "plt.scatter(x, y, s=10)\n",
    "\n",
    "# # sort the values of x before line plot\n",
    "sort_axis = operator.itemgetter(0)\n",
    "sorted_zip = sorted(zip(x_test,y_poly_preds), key=sort_axis)\n",
    "x_plot, y_plot = zip(*sorted_zip)\n",
    "plt.scatter(x_plot, y_plot, color='m', s=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Bonus) Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "plt.scatter(x, y, s=10)\n",
    "\n",
    "# sort the values of x before line plot\n",
    "sort_axis = operator.itemgetter(0)\n",
    "sorted_zip = sorted(zip(x_valid,y_poly_preds), key=sort_axis)\n",
    "x_plot, y_plot = zip(*sorted_zip)\n",
    "plt.plot(x_plot, y_plot, color='m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (8) Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (TODO) What can we conclude from this analysis? Write a little description summarizing the findings and how we approach a final solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (NO NEED TO LOOK BELOW HERE!) The rest? Is trial-and-error. Wish to follow the schedule delined in order to exploit a full end-to-end ML analysis!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here below we want to take a step ahead: we want to define a function to perform parameter optimization for the polynomial-degree hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-validation set split\n",
    "X_train,X_valid,y_train,y_valid = train_test_split(df,df.y,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try with missing values\n",
    "# Retrieve the columns that have missings (all of them)\n",
    "cols_with_missings = [col for col in df.columns if df[col].isnull().any()]\n",
    "\n",
    "# Explicative situation for each column regarding missing values\n",
    "missing_values_count_by_cols = (df.isnull().sum())\n",
    "print(missing_values_count_by_cols[missing_values_count_by_cols > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Just drop values from a SUBSET!\n",
    "# df.dropna(axis=0,subset=['x'],inplace=True) # drops all the observations for which \"x\" is null\n",
    "# df.dropna(axis=0,subset=['y'],inplace=True) # drops all the observations for which \"y\" is null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## IMPUTING VALUES\n",
    "# from sklearn.impute import SimpleImputer\n",
    "\n",
    "# # Imputed values\n",
    "# my_imputer = SimpleImputer()\n",
    "# imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\n",
    "# imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))\n",
    "\n",
    "# # Restore column names\n",
    "# imputed_X_train.columns = X_train.columns\n",
    "# imputed_X_valid.columns = X_valid.columns"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}